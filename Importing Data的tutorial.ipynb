{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "### 1.基本介绍\n",
    "Tensorflow提供的tf.data接口，允许你通过简单可重复使用的部件构建一个复杂的输入流水线。\n",
    "<br>(翻译成流水线感觉有点怪，我也不知道Input Pipeline该怎么翻译好)\n",
    "<br>例如，一个图片模型的输入，也许会从分布式系统中合并文件后读取，在每个图片上加入随机干扰，然后随机选择图片加入到一个batch中进行训练。下一个模型的流水线或许设计从原始数据中提取符号，将它们通过查找表转化为<a>Embedding Identifiers</a>,将不同长度的序列合为一个batch。\n",
    "<br>tf.data使处理大量数据，不同的数据形式，以及复杂的转化方式变得容易。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.data 为开发人员提供两个抽象接口：\n",
    "<BR> 一个tf.data.Dataset代表一系列的元素，其中每个元素包含一个或者多个<a>Tensor</a>对象.\n",
    "<br> 例如，一个图像流水线，一个元素或许是一个训练样例，其中包含一对Tensor，分别代表图片数据以及一个标签，有两种不同的方式来创建一个数据集：\n",
    "* 创建一个数据源(e.g.Dataset.from_tensor_slices())，通过tf.Tensor对象来构建数据集。\n",
    "* 应用转换(e.g.Dataset.batch())从tf.data.Dataset对象中构件数据集\n",
    "\n",
    "此外，tf.data.Iterator提供从数据集提取元素的主要方式，当该方法被执行时，通过Iterator.get_next()返回数据集的下一个元素。在输入流水线意见你的模型之间，最简单的迭代器是“one-shor iterator”,它与特定的数据集绑定后，迭代运行一次。对于更多复杂的使用方法，Iterator.initializer操作使你可以在不同的数据集上重新初始化迭代器，例如，在同一个程序上，迭代训练集和验证集多次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.基本机制\n",
    "接下来这部分介绍关于创建不同类型Dataset以及Iterator的基本部件，以及如何从它们提取数据。\n",
    "<BR>\n",
    "开始一个输入流水线，首先要定义一个数据源，例如，从内存中的Tensor构建一个数据集，一个可以使用tf.data.Dataset.from_tensors()或者tf.data.Dataset.from_tensor_slices()。再或者，如果你的输入数据以TFRecord格式存储在磁盘中，你可以创建tf.data.TFRecordDataset.\n",
    "<BR>\n",
    "一旦你有一个Dataset对象，你可以将它转化为新的数据集通过tf.data.Dataset对象提供的一系列方法。例如，你可以在每个元素上应用Dataset.map(),也可以一次性转换多个元素(Dataset.batch()).更多方法参考<a href = \"https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=zh-cn\">tf.data.Dataset</a>\n",
    "<BR>\n",
    "最常见的使用数据集的方法是，实现一个Iterator对象，每次提供数据集的一个元素；tf.data.Iterator提供两种操作：\n",
    "* Iterator.initializer:初始化Iterator状态\n",
    "* Iterator.get_next():返回tf.Tensors对象，包含数据集的下一个元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 数据集结构\n",
    "每个数据集包含的元素具有类似的结构。一个元素具有一个或者多个tf.Tensor对象，称为组件。每个组件具有一个tf.Dtype表示Tensor包含的元素的数据类型，tf.TensorShape表示每个元素的静态大小。\n",
    "<br>Dataset.output_type以及Dataset.output_shapes方法允许我们查看数据集的数据类型以及大小，示例代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，每个元素可以包含一个或者多个包装在元组中的tensors，甚至是压缩的多个Tensors。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.int32)\n",
      "(TensorShape([]), TensorShape([Dimension(100)]))\n",
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
     ]
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，如果有需要的话，可以给element的每个部件命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': tf.float32, 'b': tf.int32}\n",
      "{'a': TensorShape([]), 'b': TensorShape([Dimension(100)])}\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以及，将当前数据集通过调用Dataset.map(),Dataset.flat_map()以及Dataset.filter()等方法，转换为自己需要的数据集（注意，该转换应用在每个元素）\n",
    "示例伪代码如下👇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset1 = dataset1.map(lambda x: ...)\n",
    "\n",
    "dataset2 = dataset2.flat_map(lambda x, y: ...)\n",
    "\n",
    "dataset3 = dataset3.filter(lambda x, (y, z): ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 创建迭代器\n",
    "一旦你已经创建一个数据集来表示你的输入数据，那么下一步就是创建一个迭代器来获取数据集的元素。tf.data的API接口目前支持一下迭代器，复杂程度依次递增：\n",
    "* one-shot\n",
    "* initializable\n",
    "* reinitializable\n",
    "* feedable\n",
    "(这篇tutorial仅介绍最常用的one-shot Iterator，其他请查看<a href=\"https://www.tensorflow.org/programmers_guide/datasets?hl=zh-cn\">官方文档</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-shot\n",
    "one-shot迭代器是最简单的一种，它仅支持迭代一次数据，没有特定的初始化，one-shot迭代器几乎可以处理所有基于队列的输入形式，但是它们不支持参数。\n",
    "<br>示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "sess = tf.Session()\n",
    "for i in range(10):\n",
    "    value = sess.run(next_element)\n",
    "    print(value)\n",
    "    assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✨提示：目前来说，one-shot是唯一一个在Estimator中可使用的迭代器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 使用迭代器的返回值\n",
    "Iterator.get_next()方法返回一个或者多个tf.Tensors对象，对应相应的下一个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(5)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Typically `result` will be the output of a model, or an optimizer's\n",
    "# training operation.\n",
    "result = tf.add(next_element, next_element)\n",
    "\n",
    "sess.run(iterator.initializer)\n",
    "print(sess.run(result))  # ==> \"0\"\n",
    "print(sess.run(result))  # ==> \"2\"\n",
    "print(sess.run(result))  # ==> \"4\"\n",
    "print(sess.run(result))  # ==> \"6\"\n",
    "print(sess.run(result))  # ==> \"8\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果迭代器到达数据底部，再执行Iterator.get_next()会引发tf.errors.OutOfRangeError错误。在这一点后，迭代器将出于不可用的状态，如果你想在未来继续使用必须再次初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sess.run(result)\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print(\"End of dataset\")  # ==> \"End of dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "换一种更为常见的写法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "sess.run(iterator.initializer)\n",
    "while True:\n",
    "    try:\n",
    "        print(sess.run(result))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果数据集的每个元素都有嵌套结构，那么Iterator.get_next()的返回值将是多个同一嵌套结构的tf.Tensors："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext_10:0' shape=(10,) dtype=float32>, (<tf.Tensor 'IteratorGetNext_10:1' shape=() dtype=float32>, <tf.Tensor 'IteratorGetNext_10:2' shape=(100,) dtype=float32>))\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices((tf.random_uniform([4]), tf.random_uniform([4, 100])))\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "iterator = dataset3.make_initializable_iterator()\n",
    "\n",
    "sess.run(iterator.initializer)\n",
    "test = iterator.get_next()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.读入输入数据\n",
    "#### 3.1 使用Numpy数组\n",
    "如果你的所有输入数据都已存入在内存，最简单的创建数据集的方法就是，用Dataset.from_tensor_slices()将它们转化为tf.Tensors对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load the training data into two NumPy arrays, for example using `np.load()`.\n",
    "with np.load(\"/var/data/training_data.npy\") as data:\n",
    "    features = data[\"features\"]\n",
    "    labels = data[\"labels\"]\n",
    "\n",
    "# Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意以上代码片段将features和labels数列作为tf.constant()嵌入在你的计算图(<a href=\"\">graph</a>)中.在一个小的数据集中可以工作良好，但是浪费内存----因为数列中的内容需要被复制几次，且tf.GraphDef协议限制缓冲最大空间为2GB。\n",
    "<BR>\n",
    "作为替换,你可以定义Dataset为tf.placeholder(),在初始化数据集的Iterator时，再喂入Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the training data into two NumPy arrays, for example using `np.load()`.\n",
    "with np.load(\"/var/data/training_data.npy\") as data:\n",
    "    features = data[\"features\"]\n",
    "    labels = data[\"labels\"]\n",
    "\n",
    "# Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "# [Other transformations on `dataset`...]\n",
    "dataset = ...\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: features,\n",
    "                                          labels_placeholder: labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 使用TFRecord数据\n",
    "tf.data提供对各种文件格式的支持，所以你可以处理在内存中无法存下的大规模数据集。例如，TFRecord文件格式是一种简单的二进制格式，tf.data.TFRecordDataset类允许你使用一个或多个TFRecord文件作为输入流水线的一部分。\n",
    "<br>示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a dataset that reads all of the examples from two files.\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3使用文本数据\n",
    "许多数据集都分布在一个或者多个text文件中。tf.data.TextLIneDataset提供一种简易的方式从一个或者多个text文件中读入行数据。假如有一个或者多个文件，一个TextLineDataset将会为每一个文件产生一行String元素。TextLineDataset接收文件名字作为tf.Tensor,所以你可以通过将其传入tf.placeholder(tf.string)来参数化。\n",
    "<br>示例如下 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\"]\n",
    "dataset = tf.data.TextLineDataset(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认来说，一个TextLineDataset以行读入文件，但或许太过理想化，例如文件开始于标题行，或者包含一些评论，你可以通过调用Dataset.skip()以及Dataset.filter()来移除这些行，将这些转化分别应用在读入的文件中，我们可以调用Dataset.flat_map()方法，来为每个文件创建一个压缩版数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\"]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "\n",
    "# Use `Dataset.flat_map()` to transform each file as a separate nested dataset,\n",
    "# and then concatenate their contents sequentially into a single \"flat\" dataset.\n",
    "# * Skip the first line (header row).\n",
    "# * Filter out lines beginning with \"#\" (comments).\n",
    "dataset = dataset.flat_map(\n",
    "    lambda filename: (\n",
    "        tf.data.TextLineDataset(filename)\n",
    "        .skip(1)\n",
    "        .filter(lambda line: tf.not_equal(tf.substr(line, 0, 1), \"#\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.用Dataset.map()预处理数据\n",
    "Dataset.map(f)转化函数通过在输入数据集的每个元素上应用给定的函数f来产生新的数据集，这基于<a href = \"https://en.wikipedia.org/wiki/Map_(higher-order_function)\">map</a>，是一种常见的函数编程语言，函数f获取一个输入单元元素的tf.Tensors对象，并返回新数据集的单独元素。它的实现使用了标准的Tensorflow操作，从一个元素转化为另一个。\n",
    "<BR>这一部分内容主要介绍如何使用Dataset.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 解析tf.Example协议缓存信息\n",
    "许多输入流水线从TFRecord格式的文件中提取tf.train.Example协议缓存信息。每一个tf.train.Example记录包含一个或者多个“特征”，之后输入流水线通常会将这些特征转换为Tensors.\n",
    "<br>示例代码如下👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforms a scalar string `example_proto` into a pair of a scalar string and\n",
    "# a scalar integer, representing an image and its label, respectively.\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"image\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "              \"label\": tf.FixedLenFeature((), tf.int32, default_value=0)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"image\"], parsed_features[\"label\"]\n",
    "\n",
    "# Creates a dataset that reads all of the examples from two files, and extracts\n",
    "# the image and label features.\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2解码图片数据集并重写调整大小\n",
    "当用真实世界图片集来训练神经网络是，将不同大小的图片调整为相同大小显然是极其必要的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "# to a fixed shape.\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_image(image_string)\n",
    "    image_resized = tf.image.resize_images(image_decoded, [28, 28])\n",
    "    return image_resized, label\n",
    "\n",
    "# A vector of filenames.\n",
    "filenames = tf.constant([\"/var/data/image1.jpg\", \"/var/data/image2.jpg\", ...])\n",
    "\n",
    "# `labels[i]` is the label for the image in `filenames[i].\n",
    "labels = tf.constant([0, 37, ...])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "dataset = dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者依靠外部数据库来转化图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Use a custom OpenCV function to read the image, instead of the standard\n",
    "# TensorFlow `tf.read_file()` operation.\n",
    "def _read_py_function(filename, label):\n",
    "    image_decoded = cv2.imread(filename.decode(), cv2.IMREAD_GRAYSCALE)\n",
    "    return image_decoded, label\n",
    "\n",
    "# Use standard TensorFlow operations to resize the image to a fixed shape.\n",
    "def _resize_function(image_decoded, label):\n",
    "    image_decoded.set_shape([None, None, None])\n",
    "    image_resized = tf.image.resize_images(image_decoded, [28, 28])\n",
    "    return image_resized, label\n",
    "\n",
    "filenames = [\"/var/data/image1.jpg\", \"/var/data/image2.jpg\", ...]\n",
    "labels = [0, 37, 29, 1, ...]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "dataset = dataset.map(\n",
    "    lambda filename, label: tuple(tf.py_func(\n",
    "        _read_py_function, [filename, label], [tf.uint8, label.dtype])))\n",
    "dataset = dataset.map(_resize_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.数据集的批处理\n",
    "#### 5.1 简单的批处理\n",
    "最简单的批处理就是将n个数据集中连续元素堆叠为一个元素。Dataset.batch（）就是这样做的转化工作，注意每个元素必须是完全相同的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3]), array([ 0, -1, -2, -3]))\n",
      "(array([4, 5, 6, 7]), array([-4, -5, -6, -7]))\n",
      "(array([ 8,  9, 10, 11]), array([ -8,  -9, -10, -11]))\n"
     ]
    }
   ],
   "source": [
    "inc_dataset = tf.data.Dataset.range(100)\n",
    "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
    "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
    "batched_dataset = dataset.batch(4)\n",
    "\n",
    "iterator = batched_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "print(sess.run(next_element))  # ==> ([0, 1, 2,   3],   [ 0, -1,  -2,  -3])\n",
    "print(sess.run(next_element))  # ==> ([4, 5, 6,   7],   [-4, -5,  -6,  -7])\n",
    "print(sess.run(next_element))  # ==> ([8, 9, 10, 11],   [-8, -9, -10, -11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 不那么简单的批处理\n",
    "恩上面的代码呢，要求每个元素必须完全相同大小，但实际情况中呢，许多模型的输入数据大小并不一定相同（例如不同长度的短句）。为了解决这个问题，Dataset.padded_batch()方法使你可以一次性批处理不同大小的tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 0 0]\n",
      " [2 2 0]\n",
      " [3 3 3]]\n",
      "[[4 4 4 4 0 0 0]\n",
      " [5 5 5 5 5 0 0]\n",
      " [6 6 6 6 6 6 0]\n",
      " [7 7 7 7 7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "dataset = dataset.padded_batch(4, padded_shapes=[None])\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "print(sess.run(next_element))  # ==> [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]\n",
    "print(sess.run(next_element))  # ==> [[4, 4, 4, 4, 0, 0, 0],\n",
    "                               #      [5, 5, 5, 5, 5, 0, 0],\n",
    "                               #      [6, 6, 6, 6, 6, 6, 0],\n",
    "                               #      [7, 7, 7, 7, 7, 7, 7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:Dataset.padded_batch()方法允许你为每个维度的每个组件设置不同的填充，可以使可变的长度（None），也可以是定值。\n",
    "<br>\n",
    "(It is also possible to override the padding value, which defaults to 0.这句话不知道怎么翻译没有写)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.训练工作流\n",
    "#### 6.1 Processing Multiple Epochs\n",
    "tf.data提供两种主要方式来处理多次epochs训练\n",
    "* 最简单的方式是使用Dataset.repeat()转化，例如，创建一个数据集重复10个epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.repeat(10)\n",
    "dataset = dataset.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应用Dataset.repeat()，如果不设置的参数的话，将一直重复下去。且该方法在训练前后两个epoch时，不会在第一个epoch结束后发出信号.\n",
    "<br>如果你想在每个epoch结束时收到信号，你可以写一个训练循环来捕捉tf.errors.OutOfRangeError在每个数据集的最后。在这一点你可能会收集到一些关于当前epoch的统计信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.batch(32)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Compute for 100 epochs.\n",
    "for _ in range(100):\n",
    "    sess.run(iterator.initializer)\n",
    "    while True:\n",
    "    try:\n",
    "          sess.run(next_element)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "          break\n",
    "\n",
    "  # [Perform end-of-epoch calculations here.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Randomly shuffling input data\n",
    "(其实是打乱输入数据顺序的意思，但我不知道怎么翻译比较文雅)\n",
    "<BR>\n",
    "Dataset.shuffle()方法随机打乱输入数据用一个类似于tf.RandomShuffleQueue的算法：它维护一个固定大小的缓存，并且从缓存中随机选取下一个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 调用高层接口\n",
    " tf.train.MonitoredTrainingSession简化了分布式运行Tensorflow的设置，MonitoredTrainingSession调用tf.errors.OutOfRangeError来提示训练已经结束，所以为了和tf.data接口能够兼容使用，我们(Tensorflow的开发员们)推荐使用Dataset.make_one_shot_iterator().例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.repeat(num_epochs)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "next_example, next_label = iterator.get_next()\n",
    "loss = model_function(next_example, next_label)\n",
    "\n",
    "training_op = tf.train.AdagradOptimizer(...).minimize(loss)\n",
    "\n",
    "with tf.train.MonitoredTrainingSession(...) as sess:\n",
    "    while not sess.should_stop():\n",
    "        sess.run(training_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了在Estimator的input_fn函数中使用Dataset，我们也推荐使用Dataset.make_one_shot_iterator()，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataset_input_fn():\n",
    "    filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "  # Use `tf.parse_single_example()` to extract data from a `tf.Example`\n",
    "  # protocol buffer, and perform any additional per-record preprocessing.\n",
    "    def parser(record):\n",
    "        keys_to_features = {\n",
    "        \"image_data\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "        \"date_time\": tf.FixedLenFeature((), tf.int64, default_value=\"\"),\n",
    "        \"label\": tf.FixedLenFeature((), tf.int64,\n",
    "                                    default_value=tf.zeros([], dtype=tf.int64)),\n",
    "        }\n",
    "        parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    # Perform additional preprocessing on the parsed data.\n",
    "        image = tf.image.decode_jpeg(parsed[\"image_data\"])\n",
    "        image = tf.reshape(image, [299, 299, 1])\n",
    "        label = tf.cast(parsed[\"label\"], tf.int32)\n",
    "\n",
    "        return {\"image_data\": image, \"date_time\": parsed[\"date_time\"]}, label\n",
    "\n",
    "  # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "  # tensor for each example.\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "  # `features` is a dictionary in which each value is a batch of values for\n",
    "  # that feature; `labels` is a batch of labels.\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，今日份的tutorial结束了，不得不说太长了，翻译起来真是麻烦T T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
