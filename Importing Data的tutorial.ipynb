{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "### 1.åŸºæœ¬ä»‹ç»\n",
    "Tensorflowæä¾›çš„tf.dataæ¥å£ï¼Œå…è®¸ä½ é€šè¿‡ç®€å•å¯é‡å¤ä½¿ç”¨çš„éƒ¨ä»¶æ„å»ºä¸€ä¸ªå¤æ‚çš„è¾“å…¥æµæ°´çº¿ã€‚\n",
    "<br>(ç¿»è¯‘æˆæµæ°´çº¿æ„Ÿè§‰æœ‰ç‚¹æ€ªï¼Œæˆ‘ä¹Ÿä¸çŸ¥é“Input Pipelineè¯¥æ€ä¹ˆç¿»è¯‘å¥½)\n",
    "<br>ä¾‹å¦‚ï¼Œä¸€ä¸ªå›¾ç‰‡æ¨¡å‹çš„è¾“å…¥ï¼Œä¹Ÿè®¸ä¼šä»åˆ†å¸ƒå¼ç³»ç»Ÿä¸­åˆå¹¶æ–‡ä»¶åè¯»å–ï¼Œåœ¨æ¯ä¸ªå›¾ç‰‡ä¸ŠåŠ å…¥éšæœºå¹²æ‰°ï¼Œç„¶åéšæœºé€‰æ‹©å›¾ç‰‡åŠ å…¥åˆ°ä¸€ä¸ªbatchä¸­è¿›è¡Œè®­ç»ƒã€‚ä¸‹ä¸€ä¸ªæ¨¡å‹çš„æµæ°´çº¿æˆ–è®¸è®¾è®¡ä»åŸå§‹æ•°æ®ä¸­æå–ç¬¦å·ï¼Œå°†å®ƒä»¬é€šè¿‡æŸ¥æ‰¾è¡¨è½¬åŒ–ä¸º<a>Embedding Identifiers</a>,å°†ä¸åŒé•¿åº¦çš„åºåˆ—åˆä¸ºä¸€ä¸ªbatchã€‚\n",
    "<br>tf.dataä½¿å¤„ç†å¤§é‡æ•°æ®ï¼Œä¸åŒçš„æ•°æ®å½¢å¼ï¼Œä»¥åŠå¤æ‚çš„è½¬åŒ–æ–¹å¼å˜å¾—å®¹æ˜“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.data ä¸ºå¼€å‘äººå‘˜æä¾›ä¸¤ä¸ªæŠ½è±¡æ¥å£ï¼š\n",
    "<BR> ä¸€ä¸ªtf.data.Datasetä»£è¡¨ä¸€ç³»åˆ—çš„å…ƒç´ ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ åŒ…å«ä¸€ä¸ªæˆ–è€…å¤šä¸ª<a>Tensor</a>å¯¹è±¡.\n",
    "<br> ä¾‹å¦‚ï¼Œä¸€ä¸ªå›¾åƒæµæ°´çº¿ï¼Œä¸€ä¸ªå…ƒç´ æˆ–è®¸æ˜¯ä¸€ä¸ªè®­ç»ƒæ ·ä¾‹ï¼Œå…¶ä¸­åŒ…å«ä¸€å¯¹Tensorï¼Œåˆ†åˆ«ä»£è¡¨å›¾ç‰‡æ•°æ®ä»¥åŠä¸€ä¸ªæ ‡ç­¾ï¼Œæœ‰ä¸¤ç§ä¸åŒçš„æ–¹å¼æ¥åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ï¼š\n",
    "* åˆ›å»ºä¸€ä¸ªæ•°æ®æº(e.g.Dataset.from_tensor_slices())ï¼Œé€šè¿‡tf.Tensorå¯¹è±¡æ¥æ„å»ºæ•°æ®é›†ã€‚\n",
    "* åº”ç”¨è½¬æ¢(e.g.Dataset.batch())ä»tf.data.Datasetå¯¹è±¡ä¸­æ„ä»¶æ•°æ®é›†\n",
    "\n",
    "æ­¤å¤–ï¼Œtf.data.Iteratoræä¾›ä»æ•°æ®é›†æå–å…ƒç´ çš„ä¸»è¦æ–¹å¼ï¼Œå½“è¯¥æ–¹æ³•è¢«æ‰§è¡Œæ—¶ï¼Œé€šè¿‡Iterator.get_next()è¿”å›æ•°æ®é›†çš„ä¸‹ä¸€ä¸ªå…ƒç´ ã€‚åœ¨è¾“å…¥æµæ°´çº¿æ„è§ä½ çš„æ¨¡å‹ä¹‹é—´ï¼Œæœ€ç®€å•çš„è¿­ä»£å™¨æ˜¯â€œone-shor iteratorâ€,å®ƒä¸ç‰¹å®šçš„æ•°æ®é›†ç»‘å®šåï¼Œè¿­ä»£è¿è¡Œä¸€æ¬¡ã€‚å¯¹äºæ›´å¤šå¤æ‚çš„ä½¿ç”¨æ–¹æ³•ï¼ŒIterator.initializeræ“ä½œä½¿ä½ å¯ä»¥åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šé‡æ–°åˆå§‹åŒ–è¿­ä»£å™¨ï¼Œä¾‹å¦‚ï¼Œåœ¨åŒä¸€ä¸ªç¨‹åºä¸Šï¼Œè¿­ä»£è®­ç»ƒé›†å’ŒéªŒè¯é›†å¤šæ¬¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.åŸºæœ¬æœºåˆ¶\n",
    "æ¥ä¸‹æ¥è¿™éƒ¨åˆ†ä»‹ç»å…³äºåˆ›å»ºä¸åŒç±»å‹Datasetä»¥åŠIteratorçš„åŸºæœ¬éƒ¨ä»¶ï¼Œä»¥åŠå¦‚ä½•ä»å®ƒä»¬æå–æ•°æ®ã€‚\n",
    "<BR>\n",
    "å¼€å§‹ä¸€ä¸ªè¾“å…¥æµæ°´çº¿ï¼Œé¦–å…ˆè¦å®šä¹‰ä¸€ä¸ªæ•°æ®æºï¼Œä¾‹å¦‚ï¼Œä»å†…å­˜ä¸­çš„Tensoræ„å»ºä¸€ä¸ªæ•°æ®é›†ï¼Œä¸€ä¸ªå¯ä»¥ä½¿ç”¨tf.data.Dataset.from_tensors()æˆ–è€…tf.data.Dataset.from_tensor_slices()ã€‚å†æˆ–è€…ï¼Œå¦‚æœä½ çš„è¾“å…¥æ•°æ®ä»¥TFRecordæ ¼å¼å­˜å‚¨åœ¨ç£ç›˜ä¸­ï¼Œä½ å¯ä»¥åˆ›å»ºtf.data.TFRecordDataset.\n",
    "<BR>\n",
    "ä¸€æ—¦ä½ æœ‰ä¸€ä¸ªDatasetå¯¹è±¡ï¼Œä½ å¯ä»¥å°†å®ƒè½¬åŒ–ä¸ºæ–°çš„æ•°æ®é›†é€šè¿‡tf.data.Datasetå¯¹è±¡æä¾›çš„ä¸€ç³»åˆ—æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥åœ¨æ¯ä¸ªå…ƒç´ ä¸Šåº”ç”¨Dataset.map(),ä¹Ÿå¯ä»¥ä¸€æ¬¡æ€§è½¬æ¢å¤šä¸ªå…ƒç´ (Dataset.batch()).æ›´å¤šæ–¹æ³•å‚è€ƒ<a href = \"https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=zh-cn\">tf.data.Dataset</a>\n",
    "<BR>\n",
    "æœ€å¸¸è§çš„ä½¿ç”¨æ•°æ®é›†çš„æ–¹æ³•æ˜¯ï¼Œå®ç°ä¸€ä¸ªIteratorå¯¹è±¡ï¼Œæ¯æ¬¡æä¾›æ•°æ®é›†çš„ä¸€ä¸ªå…ƒç´ ï¼›tf.data.Iteratoræä¾›ä¸¤ç§æ“ä½œï¼š\n",
    "* Iterator.initializer:åˆå§‹åŒ–IteratorçŠ¶æ€\n",
    "* Iterator.get_next():è¿”å›tf.Tensorså¯¹è±¡ï¼ŒåŒ…å«æ•°æ®é›†çš„ä¸‹ä¸€ä¸ªå…ƒç´ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 æ•°æ®é›†ç»“æ„\n",
    "æ¯ä¸ªæ•°æ®é›†åŒ…å«çš„å…ƒç´ å…·æœ‰ç±»ä¼¼çš„ç»“æ„ã€‚ä¸€ä¸ªå…ƒç´ å…·æœ‰ä¸€ä¸ªæˆ–è€…å¤šä¸ªtf.Tensorå¯¹è±¡ï¼Œç§°ä¸ºç»„ä»¶ã€‚æ¯ä¸ªç»„ä»¶å…·æœ‰ä¸€ä¸ªtf.Dtypeè¡¨ç¤ºTensoråŒ…å«çš„å…ƒç´ çš„æ•°æ®ç±»å‹ï¼Œtf.TensorShapeè¡¨ç¤ºæ¯ä¸ªå…ƒç´ çš„é™æ€å¤§å°ã€‚\n",
    "<br>Dataset.output_typeä»¥åŠDataset.output_shapesæ–¹æ³•å…è®¸æˆ‘ä»¬æŸ¥çœ‹æ•°æ®é›†çš„æ•°æ®ç±»å‹ä»¥åŠå¤§å°ï¼Œç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤å¤–ï¼Œæ¯ä¸ªå…ƒç´ å¯ä»¥åŒ…å«ä¸€ä¸ªæˆ–è€…å¤šä¸ªåŒ…è£…åœ¨å…ƒç»„ä¸­çš„tensorsï¼Œç”šè‡³æ˜¯å‹ç¼©çš„å¤šä¸ªTensorsã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.int32)\n",
      "(TensorShape([]), TensorShape([Dimension(100)]))\n",
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
     ]
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŒæ—¶ï¼Œå¦‚æœæœ‰éœ€è¦çš„è¯ï¼Œå¯ä»¥ç»™elementçš„æ¯ä¸ªéƒ¨ä»¶å‘½å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': tf.float32, 'b': tf.int32}\n",
      "{'a': TensorShape([]), 'b': TensorShape([Dimension(100)])}\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥åŠï¼Œå°†å½“å‰æ•°æ®é›†é€šè¿‡è°ƒç”¨Dataset.map(),Dataset.flat_map()ä»¥åŠDataset.filter()ç­‰æ–¹æ³•ï¼Œè½¬æ¢ä¸ºè‡ªå·±éœ€è¦çš„æ•°æ®é›†ï¼ˆæ³¨æ„ï¼Œè¯¥è½¬æ¢åº”ç”¨åœ¨æ¯ä¸ªå…ƒç´ ï¼‰\n",
    "ç¤ºä¾‹ä¼ªä»£ç å¦‚ä¸‹ğŸ‘‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset1 = dataset1.map(lambda x: ...)\n",
    "\n",
    "dataset2 = dataset2.flat_map(lambda x, y: ...)\n",
    "\n",
    "dataset3 = dataset3.filter(lambda x, (y, z): ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 åˆ›å»ºè¿­ä»£å™¨\n",
    "ä¸€æ—¦ä½ å·²ç»åˆ›å»ºä¸€ä¸ªæ•°æ®é›†æ¥è¡¨ç¤ºä½ çš„è¾“å…¥æ•°æ®ï¼Œé‚£ä¹ˆä¸‹ä¸€æ­¥å°±æ˜¯åˆ›å»ºä¸€ä¸ªè¿­ä»£å™¨æ¥è·å–æ•°æ®é›†çš„å…ƒç´ ã€‚tf.dataçš„APIæ¥å£ç›®å‰æ”¯æŒä¸€ä¸‹è¿­ä»£å™¨ï¼Œå¤æ‚ç¨‹åº¦ä¾æ¬¡é€’å¢ï¼š\n",
    "* one-shot\n",
    "* initializable\n",
    "* reinitializable\n",
    "* feedable\n",
    "(è¿™ç¯‡tutorialä»…ä»‹ç»æœ€å¸¸ç”¨çš„one-shot Iteratorï¼Œå…¶ä»–è¯·æŸ¥çœ‹<a href=\"https://www.tensorflow.org/programmers_guide/datasets?hl=zh-cn\">å®˜æ–¹æ–‡æ¡£</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-shot\n",
    "one-shotè¿­ä»£å™¨æ˜¯æœ€ç®€å•çš„ä¸€ç§ï¼Œå®ƒä»…æ”¯æŒè¿­ä»£ä¸€æ¬¡æ•°æ®ï¼Œæ²¡æœ‰ç‰¹å®šçš„åˆå§‹åŒ–ï¼Œone-shotè¿­ä»£å™¨å‡ ä¹å¯ä»¥å¤„ç†æ‰€æœ‰åŸºäºé˜Ÿåˆ—çš„è¾“å…¥å½¢å¼ï¼Œä½†æ˜¯å®ƒä»¬ä¸æ”¯æŒå‚æ•°ã€‚\n",
    "<br>ç¤ºä¾‹å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "sess = tf.Session()\n",
    "for i in range(10):\n",
    "    value = sess.run(next_element)\n",
    "    print(value)\n",
    "    assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ¨æç¤ºï¼šç›®å‰æ¥è¯´ï¼Œone-shotæ˜¯å”¯ä¸€ä¸€ä¸ªåœ¨Estimatorä¸­å¯ä½¿ç”¨çš„è¿­ä»£å™¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 ä½¿ç”¨è¿­ä»£å™¨çš„è¿”å›å€¼\n",
    "Iterator.get_next()æ–¹æ³•è¿”å›ä¸€ä¸ªæˆ–è€…å¤šä¸ªtf.Tensorså¯¹è±¡ï¼Œå¯¹åº”ç›¸åº”çš„ä¸‹ä¸€ä¸ªå…ƒç´ ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(5)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Typically `result` will be the output of a model, or an optimizer's\n",
    "# training operation.\n",
    "result = tf.add(next_element, next_element)\n",
    "\n",
    "sess.run(iterator.initializer)\n",
    "print(sess.run(result))  # ==> \"0\"\n",
    "print(sess.run(result))  # ==> \"2\"\n",
    "print(sess.run(result))  # ==> \"4\"\n",
    "print(sess.run(result))  # ==> \"6\"\n",
    "print(sess.run(result))  # ==> \"8\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœè¿­ä»£å™¨åˆ°è¾¾æ•°æ®åº•éƒ¨ï¼Œå†æ‰§è¡ŒIterator.get_next()ä¼šå¼•å‘tf.errors.OutOfRangeErroré”™è¯¯ã€‚åœ¨è¿™ä¸€ç‚¹åï¼Œè¿­ä»£å™¨å°†å‡ºäºä¸å¯ç”¨çš„çŠ¶æ€ï¼Œå¦‚æœä½ æƒ³åœ¨æœªæ¥ç»§ç»­ä½¿ç”¨å¿…é¡»å†æ¬¡åˆå§‹åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sess.run(result)\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print(\"End of dataset\")  # ==> \"End of dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¢ä¸€ç§æ›´ä¸ºå¸¸è§çš„å†™æ³•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "sess.run(iterator.initializer)\n",
    "while True:\n",
    "    try:\n",
    "        print(sess.run(result))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœæ•°æ®é›†çš„æ¯ä¸ªå…ƒç´ éƒ½æœ‰åµŒå¥—ç»“æ„ï¼Œé‚£ä¹ˆIterator.get_next()çš„è¿”å›å€¼å°†æ˜¯å¤šä¸ªåŒä¸€åµŒå¥—ç»“æ„çš„tf.Tensorsï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext_10:0' shape=(10,) dtype=float32>, (<tf.Tensor 'IteratorGetNext_10:1' shape=() dtype=float32>, <tf.Tensor 'IteratorGetNext_10:2' shape=(100,) dtype=float32>))\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices((tf.random_uniform([4]), tf.random_uniform([4, 100])))\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "iterator = dataset3.make_initializable_iterator()\n",
    "\n",
    "sess.run(iterator.initializer)\n",
    "test = iterator.get_next()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.è¯»å…¥è¾“å…¥æ•°æ®\n",
    "#### 3.1 ä½¿ç”¨Numpyæ•°ç»„\n",
    "å¦‚æœä½ çš„æ‰€æœ‰è¾“å…¥æ•°æ®éƒ½å·²å­˜å…¥åœ¨å†…å­˜ï¼Œæœ€ç®€å•çš„åˆ›å»ºæ•°æ®é›†çš„æ–¹æ³•å°±æ˜¯ï¼Œç”¨Dataset.from_tensor_slices()å°†å®ƒä»¬è½¬åŒ–ä¸ºtf.Tensorså¯¹è±¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load the training data into two NumPy arrays, for example using `np.load()`.\n",
    "with np.load(\"/var/data/training_data.npy\") as data:\n",
    "    features = data[\"features\"]\n",
    "    labels = data[\"labels\"]\n",
    "\n",
    "# Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ³¨æ„ä»¥ä¸Šä»£ç ç‰‡æ®µå°†featureså’Œlabelsæ•°åˆ—ä½œä¸ºtf.constant()åµŒå…¥åœ¨ä½ çš„è®¡ç®—å›¾(<a href=\"\">graph</a>)ä¸­.åœ¨ä¸€ä¸ªå°çš„æ•°æ®é›†ä¸­å¯ä»¥å·¥ä½œè‰¯å¥½ï¼Œä½†æ˜¯æµªè´¹å†…å­˜----å› ä¸ºæ•°åˆ—ä¸­çš„å†…å®¹éœ€è¦è¢«å¤åˆ¶å‡ æ¬¡ï¼Œä¸”tf.GraphDefåè®®é™åˆ¶ç¼“å†²æœ€å¤§ç©ºé—´ä¸º2GBã€‚\n",
    "<BR>\n",
    "ä½œä¸ºæ›¿æ¢,ä½ å¯ä»¥å®šä¹‰Datasetä¸ºtf.placeholder(),åœ¨åˆå§‹åŒ–æ•°æ®é›†çš„Iteratoræ—¶ï¼Œå†å–‚å…¥Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the training data into two NumPy arrays, for example using `np.load()`.\n",
    "with np.load(\"/var/data/training_data.npy\") as data:\n",
    "    features = data[\"features\"]\n",
    "    labels = data[\"labels\"]\n",
    "\n",
    "# Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "# [Other transformations on `dataset`...]\n",
    "dataset = ...\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: features,\n",
    "                                          labels_placeholder: labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 ä½¿ç”¨TFRecordæ•°æ®\n",
    "tf.dataæä¾›å¯¹å„ç§æ–‡ä»¶æ ¼å¼çš„æ”¯æŒï¼Œæ‰€ä»¥ä½ å¯ä»¥å¤„ç†åœ¨å†…å­˜ä¸­æ— æ³•å­˜ä¸‹çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚ä¾‹å¦‚ï¼ŒTFRecordæ–‡ä»¶æ ¼å¼æ˜¯ä¸€ç§ç®€å•çš„äºŒè¿›åˆ¶æ ¼å¼ï¼Œtf.data.TFRecordDatasetç±»å…è®¸ä½ ä½¿ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªTFRecordæ–‡ä»¶ä½œä¸ºè¾“å…¥æµæ°´çº¿çš„ä¸€éƒ¨åˆ†ã€‚\n",
    "<br>ç¤ºä¾‹å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a dataset that reads all of the examples from two files.\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3ä½¿ç”¨æ–‡æœ¬æ•°æ®\n",
    "è®¸å¤šæ•°æ®é›†éƒ½åˆ†å¸ƒåœ¨ä¸€ä¸ªæˆ–è€…å¤šä¸ªtextæ–‡ä»¶ä¸­ã€‚tf.data.TextLIneDatasetæä¾›ä¸€ç§ç®€æ˜“çš„æ–¹å¼ä»ä¸€ä¸ªæˆ–è€…å¤šä¸ªtextæ–‡ä»¶ä¸­è¯»å…¥è¡Œæ•°æ®ã€‚å‡å¦‚æœ‰ä¸€ä¸ªæˆ–è€…å¤šä¸ªæ–‡ä»¶ï¼Œä¸€ä¸ªTextLineDatasetå°†ä¼šä¸ºæ¯ä¸€ä¸ªæ–‡ä»¶äº§ç”Ÿä¸€è¡ŒStringå…ƒç´ ã€‚TextLineDatasetæ¥æ”¶æ–‡ä»¶åå­—ä½œä¸ºtf.Tensor,æ‰€ä»¥ä½ å¯ä»¥é€šè¿‡å°†å…¶ä¼ å…¥tf.placeholder(tf.string)æ¥å‚æ•°åŒ–ã€‚\n",
    "<br>ç¤ºä¾‹å¦‚ä¸‹ ğŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\"]\n",
    "dataset = tf.data.TextLineDataset(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é»˜è®¤æ¥è¯´ï¼Œä¸€ä¸ªTextLineDatasetä»¥è¡Œè¯»å…¥æ–‡ä»¶ï¼Œä½†æˆ–è®¸å¤ªè¿‡ç†æƒ³åŒ–ï¼Œä¾‹å¦‚æ–‡ä»¶å¼€å§‹äºæ ‡é¢˜è¡Œï¼Œæˆ–è€…åŒ…å«ä¸€äº›è¯„è®ºï¼Œä½ å¯ä»¥é€šè¿‡è°ƒç”¨Dataset.skip()ä»¥åŠDataset.filter()æ¥ç§»é™¤è¿™äº›è¡Œï¼Œå°†è¿™äº›è½¬åŒ–åˆ†åˆ«åº”ç”¨åœ¨è¯»å…¥çš„æ–‡ä»¶ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨Dataset.flat_map()æ–¹æ³•ï¼Œæ¥ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºä¸€ä¸ªå‹ç¼©ç‰ˆæ•°æ®é›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\"]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "\n",
    "# Use `Dataset.flat_map()` to transform each file as a separate nested dataset,\n",
    "# and then concatenate their contents sequentially into a single \"flat\" dataset.\n",
    "# * Skip the first line (header row).\n",
    "# * Filter out lines beginning with \"#\" (comments).\n",
    "dataset = dataset.flat_map(\n",
    "    lambda filename: (\n",
    "        tf.data.TextLineDataset(filename)\n",
    "        .skip(1)\n",
    "        .filter(lambda line: tf.not_equal(tf.substr(line, 0, 1), \"#\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.ç”¨Dataset.map()é¢„å¤„ç†æ•°æ®\n",
    "Dataset.map(f)è½¬åŒ–å‡½æ•°é€šè¿‡åœ¨è¾“å…¥æ•°æ®é›†çš„æ¯ä¸ªå…ƒç´ ä¸Šåº”ç”¨ç»™å®šçš„å‡½æ•°fæ¥äº§ç”Ÿæ–°çš„æ•°æ®é›†ï¼Œè¿™åŸºäº<a href = \"https://en.wikipedia.org/wiki/Map_(higher-order_function)\">map</a>ï¼Œæ˜¯ä¸€ç§å¸¸è§çš„å‡½æ•°ç¼–ç¨‹è¯­è¨€ï¼Œå‡½æ•°fè·å–ä¸€ä¸ªè¾“å…¥å•å…ƒå…ƒç´ çš„tf.Tensorså¯¹è±¡ï¼Œå¹¶è¿”å›æ–°æ•°æ®é›†çš„å•ç‹¬å…ƒç´ ã€‚å®ƒçš„å®ç°ä½¿ç”¨äº†æ ‡å‡†çš„Tensorflowæ“ä½œï¼Œä»ä¸€ä¸ªå…ƒç´ è½¬åŒ–ä¸ºå¦ä¸€ä¸ªã€‚\n",
    "<BR>è¿™ä¸€éƒ¨åˆ†å†…å®¹ä¸»è¦ä»‹ç»å¦‚ä½•ä½¿ç”¨Dataset.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 è§£ætf.Exampleåè®®ç¼“å­˜ä¿¡æ¯\n",
    "è®¸å¤šè¾“å…¥æµæ°´çº¿ä»TFRecordæ ¼å¼çš„æ–‡ä»¶ä¸­æå–tf.train.Exampleåè®®ç¼“å­˜ä¿¡æ¯ã€‚æ¯ä¸€ä¸ªtf.train.Exampleè®°å½•åŒ…å«ä¸€ä¸ªæˆ–è€…å¤šä¸ªâ€œç‰¹å¾â€ï¼Œä¹‹åè¾“å…¥æµæ°´çº¿é€šå¸¸ä¼šå°†è¿™äº›ç‰¹å¾è½¬æ¢ä¸ºTensors.\n",
    "<br>ç¤ºä¾‹ä»£ç å¦‚ä¸‹ğŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforms a scalar string `example_proto` into a pair of a scalar string and\n",
    "# a scalar integer, representing an image and its label, respectively.\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"image\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "              \"label\": tf.FixedLenFeature((), tf.int32, default_value=0)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"image\"], parsed_features[\"label\"]\n",
    "\n",
    "# Creates a dataset that reads all of the examples from two files, and extracts\n",
    "# the image and label features.\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2è§£ç å›¾ç‰‡æ•°æ®é›†å¹¶é‡å†™è°ƒæ•´å¤§å°\n",
    "å½“ç”¨çœŸå®ä¸–ç•Œå›¾ç‰‡é›†æ¥è®­ç»ƒç¥ç»ç½‘ç»œæ˜¯ï¼Œå°†ä¸åŒå¤§å°çš„å›¾ç‰‡è°ƒæ•´ä¸ºç›¸åŒå¤§å°æ˜¾ç„¶æ˜¯æå…¶å¿…è¦çš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "# to a fixed shape.\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_image(image_string)\n",
    "    image_resized = tf.image.resize_images(image_decoded, [28, 28])\n",
    "    return image_resized, label\n",
    "\n",
    "# A vector of filenames.\n",
    "filenames = tf.constant([\"/var/data/image1.jpg\", \"/var/data/image2.jpg\", ...])\n",
    "\n",
    "# `labels[i]` is the label for the image in `filenames[i].\n",
    "labels = tf.constant([0, 37, ...])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "dataset = dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ–è€…ä¾é å¤–éƒ¨æ•°æ®åº“æ¥è½¬åŒ–å›¾ç‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Use a custom OpenCV function to read the image, instead of the standard\n",
    "# TensorFlow `tf.read_file()` operation.\n",
    "def _read_py_function(filename, label):\n",
    "    image_decoded = cv2.imread(filename.decode(), cv2.IMREAD_GRAYSCALE)\n",
    "    return image_decoded, label\n",
    "\n",
    "# Use standard TensorFlow operations to resize the image to a fixed shape.\n",
    "def _resize_function(image_decoded, label):\n",
    "    image_decoded.set_shape([None, None, None])\n",
    "    image_resized = tf.image.resize_images(image_decoded, [28, 28])\n",
    "    return image_resized, label\n",
    "\n",
    "filenames = [\"/var/data/image1.jpg\", \"/var/data/image2.jpg\", ...]\n",
    "labels = [0, 37, 29, 1, ...]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "dataset = dataset.map(\n",
    "    lambda filename, label: tuple(tf.py_func(\n",
    "        _read_py_function, [filename, label], [tf.uint8, label.dtype])))\n",
    "dataset = dataset.map(_resize_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.æ•°æ®é›†çš„æ‰¹å¤„ç†\n",
    "#### 5.1 ç®€å•çš„æ‰¹å¤„ç†\n",
    "æœ€ç®€å•çš„æ‰¹å¤„ç†å°±æ˜¯å°†nä¸ªæ•°æ®é›†ä¸­è¿ç»­å…ƒç´ å †å ä¸ºä¸€ä¸ªå…ƒç´ ã€‚Dataset.batchï¼ˆï¼‰å°±æ˜¯è¿™æ ·åšçš„è½¬åŒ–å·¥ä½œï¼Œæ³¨æ„æ¯ä¸ªå…ƒç´ å¿…é¡»æ˜¯å®Œå…¨ç›¸åŒçš„å¤§å°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3]), array([ 0, -1, -2, -3]))\n",
      "(array([4, 5, 6, 7]), array([-4, -5, -6, -7]))\n",
      "(array([ 8,  9, 10, 11]), array([ -8,  -9, -10, -11]))\n"
     ]
    }
   ],
   "source": [
    "inc_dataset = tf.data.Dataset.range(100)\n",
    "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
    "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
    "batched_dataset = dataset.batch(4)\n",
    "\n",
    "iterator = batched_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "print(sess.run(next_element))  # ==> ([0, 1, 2,   3],   [ 0, -1,  -2,  -3])\n",
    "print(sess.run(next_element))  # ==> ([4, 5, 6,   7],   [-4, -5,  -6,  -7])\n",
    "print(sess.run(next_element))  # ==> ([8, 9, 10, 11],   [-8, -9, -10, -11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 ä¸é‚£ä¹ˆç®€å•çš„æ‰¹å¤„ç†\n",
    "æ©ä¸Šé¢çš„ä»£ç å‘¢ï¼Œè¦æ±‚æ¯ä¸ªå…ƒç´ å¿…é¡»å®Œå…¨ç›¸åŒå¤§å°ï¼Œä½†å®é™…æƒ…å†µä¸­å‘¢ï¼Œè®¸å¤šæ¨¡å‹çš„è¾“å…¥æ•°æ®å¤§å°å¹¶ä¸ä¸€å®šç›¸åŒï¼ˆä¾‹å¦‚ä¸åŒé•¿åº¦çš„çŸ­å¥ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒDataset.padded_batch()æ–¹æ³•ä½¿ä½ å¯ä»¥ä¸€æ¬¡æ€§æ‰¹å¤„ç†ä¸åŒå¤§å°çš„tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 0 0]\n",
      " [2 2 0]\n",
      " [3 3 3]]\n",
      "[[4 4 4 4 0 0 0]\n",
      " [5 5 5 5 5 0 0]\n",
      " [6 6 6 6 6 6 0]\n",
      " [7 7 7 7 7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "dataset = dataset.padded_batch(4, padded_shapes=[None])\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "print(sess.run(next_element))  # ==> [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]\n",
    "print(sess.run(next_element))  # ==> [[4, 4, 4, 4, 0, 0, 0],\n",
    "                               #      [5, 5, 5, 5, 5, 0, 0],\n",
    "                               #      [6, 6, 6, 6, 6, 6, 0],\n",
    "                               #      [7, 7, 7, 7, 7, 7, 7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:Dataset.padded_batch()æ–¹æ³•å…è®¸ä½ ä¸ºæ¯ä¸ªç»´åº¦çš„æ¯ä¸ªç»„ä»¶è®¾ç½®ä¸åŒçš„å¡«å……ï¼Œå¯ä»¥ä½¿å¯å˜çš„é•¿åº¦ï¼ˆNoneï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯å®šå€¼ã€‚\n",
    "<br>\n",
    "(It is also possible to override the padding value, which defaults to 0.è¿™å¥è¯ä¸çŸ¥é“æ€ä¹ˆç¿»è¯‘æ²¡æœ‰å†™)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.è®­ç»ƒå·¥ä½œæµ\n",
    "#### 6.1 Processing Multiple Epochs\n",
    "tf.dataæä¾›ä¸¤ç§ä¸»è¦æ–¹å¼æ¥å¤„ç†å¤šæ¬¡epochsè®­ç»ƒ\n",
    "* æœ€ç®€å•çš„æ–¹å¼æ˜¯ä½¿ç”¨Dataset.repeat()è½¬åŒ–ï¼Œä¾‹å¦‚ï¼Œåˆ›å»ºä¸€ä¸ªæ•°æ®é›†é‡å¤10ä¸ªepoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.repeat(10)\n",
    "dataset = dataset.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åº”ç”¨Dataset.repeat()ï¼Œå¦‚æœä¸è®¾ç½®çš„å‚æ•°çš„è¯ï¼Œå°†ä¸€ç›´é‡å¤ä¸‹å»ã€‚ä¸”è¯¥æ–¹æ³•åœ¨è®­ç»ƒå‰åä¸¤ä¸ªepochæ—¶ï¼Œä¸ä¼šåœ¨ç¬¬ä¸€ä¸ªepochç»“æŸåå‘å‡ºä¿¡å·.\n",
    "<br>å¦‚æœä½ æƒ³åœ¨æ¯ä¸ªepochç»“æŸæ—¶æ”¶åˆ°ä¿¡å·ï¼Œä½ å¯ä»¥å†™ä¸€ä¸ªè®­ç»ƒå¾ªç¯æ¥æ•æ‰tf.errors.OutOfRangeErroråœ¨æ¯ä¸ªæ•°æ®é›†çš„æœ€åã€‚åœ¨è¿™ä¸€ç‚¹ä½ å¯èƒ½ä¼šæ”¶é›†åˆ°ä¸€äº›å…³äºå½“å‰epochçš„ç»Ÿè®¡ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.batch(32)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Compute for 100 epochs.\n",
    "for _ in range(100):\n",
    "    sess.run(iterator.initializer)\n",
    "    while True:\n",
    "    try:\n",
    "          sess.run(next_element)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "          break\n",
    "\n",
    "  # [Perform end-of-epoch calculations here.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Randomly shuffling input data\n",
    "(å…¶å®æ˜¯æ‰“ä¹±è¾“å…¥æ•°æ®é¡ºåºçš„æ„æ€ï¼Œä½†æˆ‘ä¸çŸ¥é“æ€ä¹ˆç¿»è¯‘æ¯”è¾ƒæ–‡é›…)\n",
    "<BR>\n",
    "Dataset.shuffle()æ–¹æ³•éšæœºæ‰“ä¹±è¾“å…¥æ•°æ®ç”¨ä¸€ä¸ªç±»ä¼¼äºtf.RandomShuffleQueueçš„ç®—æ³•ï¼šå®ƒç»´æŠ¤ä¸€ä¸ªå›ºå®šå¤§å°çš„ç¼“å­˜ï¼Œå¹¶ä¸”ä»ç¼“å­˜ä¸­éšæœºé€‰å–ä¸‹ä¸€ä¸ªå…ƒç´ ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 è°ƒç”¨é«˜å±‚æ¥å£\n",
    " tf.train.MonitoredTrainingSessionç®€åŒ–äº†åˆ†å¸ƒå¼è¿è¡ŒTensorflowçš„è®¾ç½®ï¼ŒMonitoredTrainingSessionè°ƒç”¨tf.errors.OutOfRangeErroræ¥æç¤ºè®­ç»ƒå·²ç»ç»“æŸï¼Œæ‰€ä»¥ä¸ºäº†å’Œtf.dataæ¥å£èƒ½å¤Ÿå…¼å®¹ä½¿ç”¨ï¼Œæˆ‘ä»¬(Tensorflowçš„å¼€å‘å‘˜ä»¬)æ¨èä½¿ç”¨Dataset.make_one_shot_iterator().ä¾‹å¦‚ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.repeat(num_epochs)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "next_example, next_label = iterator.get_next()\n",
    "loss = model_function(next_example, next_label)\n",
    "\n",
    "training_op = tf.train.AdagradOptimizer(...).minimize(loss)\n",
    "\n",
    "with tf.train.MonitoredTrainingSession(...) as sess:\n",
    "    while not sess.should_stop():\n",
    "        sess.run(training_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†åœ¨Estimatorçš„input_fnå‡½æ•°ä¸­ä½¿ç”¨Datasetï¼Œæˆ‘ä»¬ä¹Ÿæ¨èä½¿ç”¨Dataset.make_one_shot_iterator()ï¼Œä¾‹å¦‚ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataset_input_fn():\n",
    "    filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "  # Use `tf.parse_single_example()` to extract data from a `tf.Example`\n",
    "  # protocol buffer, and perform any additional per-record preprocessing.\n",
    "    def parser(record):\n",
    "        keys_to_features = {\n",
    "        \"image_data\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "        \"date_time\": tf.FixedLenFeature((), tf.int64, default_value=\"\"),\n",
    "        \"label\": tf.FixedLenFeature((), tf.int64,\n",
    "                                    default_value=tf.zeros([], dtype=tf.int64)),\n",
    "        }\n",
    "        parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    # Perform additional preprocessing on the parsed data.\n",
    "        image = tf.image.decode_jpeg(parsed[\"image_data\"])\n",
    "        image = tf.reshape(image, [299, 299, 1])\n",
    "        label = tf.cast(parsed[\"label\"], tf.int32)\n",
    "\n",
    "        return {\"image_data\": image, \"date_time\": parsed[\"date_time\"]}, label\n",
    "\n",
    "  # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "  # tensor for each example.\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "  # `features` is a dictionary in which each value is a batch of values for\n",
    "  # that feature; `labels` is a batch of labels.\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¥½äº†ï¼Œä»Šæ—¥ä»½çš„tutorialç»“æŸäº†ï¼Œä¸å¾—ä¸è¯´å¤ªé•¿äº†ï¼Œç¿»è¯‘èµ·æ¥çœŸæ˜¯éº»çƒ¦T T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
