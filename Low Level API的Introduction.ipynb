{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "嗯这个Introduction吧，主要告诉你如何在不依靠Estimator的情况下，运行并管理Tensorflow程序，以及如何在这种较底层的环境使用高层组件(dataset,layers和feature columns)\n",
    "\n",
    "我们(Tensorflow的程序员们)推荐尽肯能的使用高层API来搭建模型，但了解Tensorflow的核心部分仍然是有价值的：\n",
    "* 如果你知道低层API如何工作的话，实验与debug过程都会更加直观\n",
    "* 就算你使用高层API时，也了解模型内部工作机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Setup\n",
    "在此之前，请安装Tensorflow，且需要了解以下内容：\n",
    "\n",
    "* 如何用Python编程\n",
    "* 或者至少了解一点arrays\n",
    "* 知道一点机器学习就更理想了\n",
    "\n",
    "(个人觉得还需要点线性代数知识？)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Tensor Values\n",
    "Tensor是Tensorflow中储存数据的核心，一个tensor包含一系列包装在任何维度下的数列(Tensor的阶就是所说的维度了)，而tensor的shape，表示为包装在元组中的几个整数，特指在每个维度下的数列的长度。\n",
    "\n",
    "一些示例如下👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3. # a rank 0 tensor; a scalar with shape [],\n",
    "[1., 2., 3.] # a rank 1 tensor; a vector with shape [3]\n",
    "[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]\n",
    "[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Tensorflow程序的核心流程\n",
    "你或许认为Tensorflow的核心程序包括两个独立的部分：\n",
    "* 1.建立计算图（tf.Graph）\n",
    "* 2.运行计算图（tf.Session）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.Graph\n",
    "一个计算图是指一系列的Tensorflow操作中包装在一个图中，Graph主要有两个部分组成：\n",
    "* <a href = \"\">Operation</a>(或者ops)：Graph的节点，Operation描述了使用和产生Tensor的操作\n",
    "* Tensor：Graph的边，他们表示了数据在Graph的工作流，大多数Tensorflow函数都会返回tf.Tensors\n",
    "\n",
    "Note:tf.Tensor本身并不包含数值，它们只是计算图中各个元素的<a href=\"https://www.cnblogs.com/idorax/p/6414007.html\">handle</a>\n",
    "\n",
    "我们来建立一个简单的计算图，最基础的操作是设置constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Const_1:0\", shape=(), dtype=float32)\n",
      "Tensor(\"add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(3.0, dtype=tf.float32)\n",
    "b = tf.constant(4.0) # also tf.float32 implicitly\n",
    "total = a + b\n",
    "print(a)\n",
    "print(b)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意打印出的tensor并没有输出3.0,4.0,7.0,以上部分只是建立了计算图，这些Tensor对象仅仅表示了将要被执行的操作结果。\n",
    "\n",
    "Graph中每一个操作都被赋予独一无二的名字，这个名字独立于那些在部署给python的对象的名字。Tensor在operation产生后命名，例如上面输出部分的“add:0”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. TensorBoard\n",
    "Tensorflow提供了TensorBoard，它的主要功能是可视化计算图，你只需用几个简单的指令就可以做到。\n",
    "\n",
    "* 首先，你需要将计算图保存到一个TensorBoard summary file中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "\n",
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 该操作将在当前目录下生成event文件，文件名字的格式如下：\n",
    "\n",
    "events.out.tfevents.{timestamp}.{hostname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 现在，开启一个终端，执行以下命令：\n",
    "\n",
    "tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在你的浏览器下打开TensorBoard的网址，大概会看到类似于以下的图：\n",
    "<img src=\"https://www.tensorflow.org/images/getting_started_add.png?hl=zh-cn\">\n",
    "\n",
    "关于TensorBoard的更多内容，查看<a href = “”>TensorBoard的Tutorial</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Session\n",
    "每个Session包含Tensorflow运行时的状态，并且负责执行Tensorflow的operation。打个比方来说，如果tf.Graph是一个.py文件的话，那么Session就是python的执行部分\n",
    "\n",
    "以下代码创建了一个tf.Session对象并且唤起它的run方法，来估测我们刚刚创建的名为total的Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当你调用Session.run请求输出一个节点时，Tensorflow将追溯图中所有提供所请求输出的输入，所以最终的执行结果是7\n",
    "\n",
    "你也可以向Session.run()中以字典的形式传入多个tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ab': (3.0, 4.0), 'total': 7.0}\n"
     ]
    }
   ],
   "source": [
    "print(sess.run({'ab':(a, b), 'total':total}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以及，需要注意的是，如果使用tf.random_uniform创建tensor,每次Session.run过程中都会产生不同值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03883779 0.82260215 0.79136217]\n",
      "[0.68874    0.42780805 0.7759861 ]\n",
      "(array([1.8903666, 1.4699574, 1.6417528], dtype=float32), array([2.8903666, 2.4699574, 2.6417527], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "vec = tf.random_uniform(shape=(3,))\n",
    "out1 = vec + 1\n",
    "out2 = vec + 2\n",
    "print(sess.run(vec))\n",
    "print(sess.run(vec))\n",
    "print(sess.run((out1, out2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Feeding\n",
    "如果tensorflow真的只有以上功能的话（产生定值），就太无聊啦。\n",
    "<br>事实上，一个图可以接受其他输出途径，例如tf.placeholder(),它允许我们稍后再赋予具体的值，就像一个函数的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_3:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随后，在执行，Session.run()时，我们使用feed_dict参数给placeholder喂入具体的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(z, feed_dict={x: 3, y: 4.5}))\n",
    "print(sess.run(z, feed_dict={x: [1, 3], y: [2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: feed_dict可以用于重写Graph中的任何tensor，但placeholder与其他tensor唯一的区别是：如果placeholder没有被赋予值会抛出错误异常"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dataset\n",
    "placeholder在简单的实验下比较有效，但是在向模型输入数据流时，Dataset更值得推荐。\n",
    "\n",
    "为了从Dataset中得到可执行的tf.Tensor,首先你需要将它转化为tf.data.Iterator,之后调用Iterator的get_next方法。\n",
    "\n",
    "创建Iterator最简单的方式就是用make_ont_shot_iterator方法，例如，以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_data = [\n",
    "    [0, 1,],\n",
    "    [2, 3,],\n",
    "    [4, 5,],\n",
    "    [6, 7,],\n",
    "]\n",
    "slices = tf.data.Dataset.from_tensor_slices(my_data)\n",
    "next_item = slices.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next_item每次运行时都会返回my_data中的一行数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[2 3]\n",
      "[4 5]\n",
      "[6 7]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        print(sess.run(next_item))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于Dataset的更多内容请查看<a href=“”>Importing Data的tutorial</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Layers\n",
    "（我觉得layer是什么大家都懂...\n",
    "\n",
    "#### 5.1 创建Layer\n",
    "例如创建一个全连接层，unit是指为每一个传入的vector创建输出的数量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "linear_model = tf.layers.Dense(units=1)\n",
    "y = linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 初始化Layer\n",
    "包含变量的Layer在使用之前必须初始化，但通过以下简单的代码就可以实现初始化Graph中所有变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note：\n",
    "* global_variables_initializer仅仅创建并返回tensorflow operation的handle，真正的初始化在我们执行Session.run()时完成\n",
    "* global_variables_initializer仅仅初始化当它被创建时已经存在于图中的变量，所以initializer必须是创建图过程中最后一个被创建的对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5839438]\n",
      " [5.915743 ]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(y, {x: [[1, 2, 3],[4, 5, 6]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示，layer为每个输入的vector生成一个输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Layer函数的简便版\n",
    "对于每个layer class，Tensorflow都为其创建了简便版，例如tf.layers.Dense的简便版是tf.layers.dense,区别如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -5.5061736]\n",
      " [-12.189417 ]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "###\n",
    "y = tf.layers.dense(x, units=1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run(y, {x: [[1, 2, 3], [4, 5, 6]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但缺点是debug过程更麻烦，且无法重复使用某次创建的Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Feature Columns\n",
    "使用Feature Columns最简单的方法是调用tf.feature_column.input_layer函数，但该函数仅接受Dense column，所以如果想查看categorical Column，必须要将其包装在indicator column后再传入。\n",
    "(关于dense/categorical column是什么，参考下图)\n",
    "<img src = \"https://www.tensorflow.org/images/feature_columns/some_constructors.jpg?hl=zh-cn\">\n",
    "（其实这些在<a href = \"\">Feature Column的tutorial</a>中讲的比较详细）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = {\n",
    "    'sales' : [[5], [10], [8], [9]],\n",
    "    'department': ['sports', 'sports', 'gardening', 'gardening']}\n",
    "\n",
    "department_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'department', ['sports', 'gardening'])\n",
    "department_column = tf.feature_column.indicator_column(department_column)\n",
    "\n",
    "columns = [\n",
    "    tf.feature_column.numeric_column('sales'),\n",
    "    department_column\n",
    "]\n",
    "\n",
    "inputs = tf.feature_column.input_layer(features, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行inputs tensor将会把Feature分割为一批vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征列(Feature Column)如同layer一样，有自己的内部结构和状态，所以通常来说需要初始化，Categorical Column在内部中使用查询表(<a href = \"https://www.tensorflow.org/api_docs/python/tf/contrib/lookup?hl=zh-cn\">lookup table</a>)，需要额外的初始化过程（<a href=\"https://www.tensorflow.org/api_docs/python/tf/tables_initializer?hl=zh-cn\">tf.tables_initializer</a>）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  5.]\n",
      " [ 1.  0. 10.]\n",
      " [ 0.  1.  8.]\n",
      " [ 0.  1.  9.]]\n"
     ]
    }
   ],
   "source": [
    "var_init = tf.global_variables_initializer()\n",
    "table_init = tf.tables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run((var_init, table_init))\n",
    "print(sess.run(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Training\n",
    "现在你已经熟悉Tensorflow中基本的核心概念了，现在我们来手动运行一个简单的回归模型\n",
    "\n",
    "#### Define\n",
    "首先我们来定义一些输入，以及期待的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)\n",
    "y_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model\n",
    "随后，我们定义一个仅有一个输出的线性模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = tf.layers.Dense(units=1)\n",
    "\n",
    "y_pred = linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先查看一下训练前的模型预测情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8881192]\n",
      " [-1.7762384]\n",
      " [-2.6643577]\n",
      " [-3.552477 ]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss\n",
    "为了优化模型我们来定义模型的loss，这里我们用回归问题的标准损失函数：平方差\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/losses?hl=zh-cn\">tf.losses</a>提供了一系列常用的损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5344759\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\n",
    "\n",
    "print(sess.run(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "tf.train.Optimizer提供一系列标准的优化算法. 它们通过降低损失不断改进每一个变量，最简单的优化算法是梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前这个代码已经建立了优化graph中所有必备的组件，并且返回一个可训练的operation，当运行时，opt会不断更新图中的变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.079372294\n",
      "0.078897715\n",
      "0.078426\n",
      "0.077957116\n",
      "0.077491015\n",
      "0.07702772\n",
      "0.07656719\n",
      "0.07610942\n",
      "0.07565437\n",
      "0.075202025\n",
      "0.07475242\n",
      "0.07430547\n",
      "0.07386122\n",
      "0.073419616\n",
      "0.07298063\n",
      "0.07254429\n",
      "0.072110556\n",
      "0.071679436\n",
      "0.071250856\n",
      "0.07082486\n",
      "0.07040142\n",
      "0.06998052\n",
      "0.06956211\n",
      "0.069146186\n",
      "0.06873278\n",
      "0.06832184\n",
      "0.06791337\n",
      "0.0675073\n",
      "0.0671037\n",
      "0.066702515\n",
      "0.0663037\n",
      "0.065907285\n",
      "0.06551322\n",
      "0.06512153\n",
      "0.06473218\n",
      "0.064345166\n",
      "0.063960455\n",
      "0.06357804\n",
      "0.063197926\n",
      "0.062820084\n",
      "0.062444475\n",
      "0.06207112\n",
      "0.061700035\n",
      "0.061331116\n",
      "0.060964435\n",
      "0.060599938\n",
      "0.06023762\n",
      "0.05987746\n",
      "0.059519473\n",
      "0.059163626\n",
      "0.058809903\n",
      "0.058458254\n",
      "0.058108773\n",
      "0.057761353\n",
      "0.057415977\n",
      "0.057072707\n",
      "0.056731477\n",
      "0.0563923\n",
      "0.056055117\n",
      "0.055719968\n",
      "0.055386834\n",
      "0.055055693\n",
      "0.054726537\n",
      "0.054399323\n",
      "0.05407409\n",
      "0.053750776\n",
      "0.05342941\n",
      "0.05310996\n",
      "0.05279244\n",
      "0.052476794\n",
      "0.052163057\n",
      "0.051851157\n",
      "0.051541146\n",
      "0.051233\n",
      "0.050926693\n",
      "0.050622195\n",
      "0.050319545\n",
      "0.050018698\n",
      "0.049719647\n",
      "0.049422372\n",
      "0.04912688\n",
      "0.048833158\n",
      "0.048541192\n",
      "0.048250973\n",
      "0.04796247\n",
      "0.04767572\n",
      "0.047390677\n",
      "0.04710734\n",
      "0.046825677\n",
      "0.046545714\n",
      "0.046267442\n",
      "0.04599081\n",
      "0.045715854\n",
      "0.04544252\n",
      "0.045170814\n",
      "0.04490073\n",
      "0.0446323\n",
      "0.044365443\n",
      "0.044100188\n",
      "0.04383652\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    _, loss_value = sess.run((train, loss))\n",
    "    print(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note：注意我们同时将loss放入run中是为了查看输出，因为train只是操作，并不返回具体值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
