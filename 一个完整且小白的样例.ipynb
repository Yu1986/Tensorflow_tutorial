{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬ä¸€ä¸ªæ ·ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœä½ æœ‰çœ‹TensorFlowçš„æºç çš„è¯ï¼Œå¯èƒ½ç»å¸¸ä¼šçœ‹è§è¿™ä¸‰è¡Œï¼ŒåŸå› å˜›ï¼Œè¿™æ˜¯TensorFlowå®¶è‡ªå®šçš„<a href=\"https://www.tensorflow.org/community/style_guide?hl=zh-cn\">æ–‡æ¡£è§„èŒƒ</a> \n",
    "<br>æœ‰äººå¯èƒ½å¯¹ _future_ æœ‰ç–‘é—®, ä»æ˜¯å•¥ï¼Œä¸ºå•¥ä¸¤ä¸ªæ–¹é¢æ¥å›ç­”å¥½äº†\n",
    "<br>1ï¼‰æ˜¯å•¥\n",
    "<br>ç­”ï¼š<a href = \"https://docs.python.org/3/library/__future__.html\">æ˜¯ä¸€ä¸ªå«_future_çš„moduleå‘€</a>\n",
    "<br>2) ä¸ºå•¥\n",
    "<br>ç­”ï¼šè§£å†³ç‰ˆæœ¬é—´çš„ä¸å…¼å®¹é—®é¢˜ï¼Œä¾‹å¦‚æˆ‘æ¯”è¾ƒæ‡’ï¼Œä¸‹è½½pythonä¹‹åå°±æ‡’å¾—æ›´æ–°ï¼Œå½“åˆ«äººéƒ½å·²ç»ç”¨3.99æ—¶ï¼Œæˆ‘è¿˜åœ¨ç”¨3.00ï¼Œ3.99ç‰ˆæœ¬çš„printä¹Ÿè®¸åœ¨è¾“å‡ºæ—¶ä¼šè‡ªå¸¦åè¡Œå›è½¦ï¼Œé‚£ä¹ˆæ‡’æƒ°çš„æˆ‘ï¼Œä»_future_è¿™ä¸ªmoduleä¸­å¯¼å…¥print_function,å°±å¯ä»¥ç”¨3.99ç‰ˆæœ¬è‡ªå¸¦åè¡Œå›è½¦çš„ç‚«é…·è¾“å‡ºäº†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import iris_data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°½ç®¡æ•°æ®éƒ½åœ¨<a href> iris_data</a>ä¸­é¢„å¤„ç†äº†ï¼Œæˆ‘ä»¬è¿˜æ˜¯éœ€è¦çœ‹ä¸€çœ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>120</th>\n",
       "      <th>4</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   120    4  setosa  versicolor  virginica\n",
       "0  6.4  2.8     5.6         2.2          2\n",
       "1  5.0  2.3     3.3         1.0          1\n",
       "2  4.9  2.5     4.5         1.7          2\n",
       "3  4.9  3.1     1.5         0.1          0\n",
       "4  5.7  3.8     1.7         0.3          0\n",
       "5  4.4  3.2     1.3         0.2          0\n",
       "6  5.4  3.4     1.5         0.4          0\n",
       "7  6.9  3.1     5.1         2.3          2\n",
       "8  6.7  3.1     4.4         1.4          1\n",
       "9  5.1  3.7     1.5         0.4          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "#s=requests.get(TRAIN_URL).content\n",
    "c=pd.read_csv(TRAIN_URL)\n",
    "c.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸çŸ¥é“ä¸ºä»€ä¹ˆçš„ï¼Œè¿™ä¸ªæ ‡é¢˜è¡Œä¼¼ä¹å¹¶ä¸å¯¹--\n",
    "<br>æŒ‰ç…§<a>å®˜æ–¹ç½‘ç«™çš„è§£é‡Š</a>ï¼Œè¿™äº”åˆ—åˆ†åˆ«æ˜¯ï¼š\n",
    "<br>è¼ç‰‡é•¿ï¼›è¼ç‰‡å®½ï¼›èŠ±ç“£é•¿ï¼›èŠ±ç“£å®½ï¼›ç§ç±»ï¼ˆæ ‡ç­¾ï¼‰ //æ³¨ï¼šæ ‡ç­¾æ˜¯0ï¼Œ1ï¼Œ2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ©ï¼Œæ—¢ç„¶å·²ç»é€æ¼äº†å››ä¸ªç‰¹å¾å€¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬è¦åšçš„ä»»åŠ¡å°±æ˜¯ï¼š\n",
    "<br>é€šè¿‡è¼ç‰‡é•¿ï¼Œè¼ç‰‡å®½ï¼ŒèŠ±ç“£é•¿å’ŒèŠ±ç“£å®½æ¥åˆ¤æ–­é¸¢å°¾èŠ±çš„ç§ç±» "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ‰ä¸€ç‚¹éœ€è¦è¯´æ˜ï¼Œè¯¥tutorialå»ºç«‹åœ¨<a>Estimator</a>åŸºç¡€ä¸Šå®Œæˆï¼Œå…³äºEstimatorçš„æ›´å¤šè§£é‡ŠåŠä»£ç ï¼Œå¯ä»¥å‚è€ƒæˆ‘ç¿»è¯‘çš„å¦ä¸€ä¸ªtutorial\n",
    "### é‚£ä¹ˆæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦åšä»¥ä¸‹å·¥ä½œï¼š\n",
    "#### (1)åˆ›å»ºè¾“å…¥å‡½æ•°\n",
    "#### (2)å®šä¹‰æ¨¡å‹çš„ç‰¹å¾åˆ—\n",
    "#### (3)åˆå§‹åŒ–Estimatorï¼ŒæŒ‡å®šç‰¹å¾åˆ—ä»¥åŠå„ç§è¶…å‚ï¼ˆå¦‚æœä¸çŸ¥é“è¶…å‚æ˜¯ä»€ä¹ˆçš„è¯ï¼Œç‚¹å‡»<a>è¿™é‡Œ</a>ï¼‰\n",
    "#### (4)åœ¨Estimatorä¸Šè°ƒç”¨ä¼˜åŒ–æ–¹æ³•ï¼Œä¼ é€’é€‚å½“çš„è¾“å…¥å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_evaluation_set():\n",
    "    features = {'SepalLength': np.array([6.4, 5.0]),\n",
    "                'SepalWidth':  np.array([2.8, 2.3]),\n",
    "                'PetalLength': np.array([5.6, 3.3]),\n",
    "                'PetalWidth':  np.array([2.2, 1.0])}\n",
    "    labels = np.array([2, 1])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    return dataset.shuffle(1000).repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šåªæ˜¯ä»å®˜ç½‘ä¸Šcopyä¸‹çš„æ ·ä¾‹ï¼Œé€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å…ˆè¦è·å–æ•°æ®ï¼Œåœ¨input_evaluation_set()å°†æ•°æ®åˆ†ç¦»æˆåˆé€‚çš„featureséƒ¨åˆ†ï¼ŒåŠlabeléƒ¨åˆ†ï¼Œåœ¨å°†è¿™éƒ¨åˆ†ä¼ å…¥åŒ…å«<a>tf.Dataset</a> APIçš„train_input_fn()ä¸­ï¼Œè¾“å‡ºå¯ä»¥ä½œä¸ºçœŸæ­£è®­ç»ƒè¿‡ç¨‹çš„è¾“å…¥\n",
    "<BR>è¿™é‡Œä¸ºäº†æ–¹ä¾¿èµ·è§å°±ç›´æ¥è°ƒç”¨iris_data.load_dataæ¥è·å–è®­ç»ƒåŠæµ‹è¯•é›†å•¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = iris_data.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥äº†ï¼Œæˆ‘ä»¬æ¥å®šä¹‰ç‰¹å¾åˆ—ï¼Œåœ¨çœŸå®æ•°æ®ä¸­ï¼Œç‰¹å¾åˆ—çš„å¤„ç†æ›´ä¸ºå¤æ‚ï¼Œä½†è¿™é‡Œåªæ˜¯ä¸¾ä¸ªğŸŒ° \n",
    "<br> ä»¥åŠï¼Œè¿™é‡Œç”¨åˆ°tf.feature_columnæ¥å¤„ç†ï¼Œå…·ä½“è¯·å‚ç…§é¢†ä¸€ä»½tutorial<a> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ å¯èƒ½æƒ³çœ‹ä¸€ä¸‹ç‰¹å¾åˆ—ä¸­éƒ½æœ‰å•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='PetalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='PetalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥æ˜¯ç¬¬ä¸‰æ­¥ï¼Œå®šä¹‰ä¸€ä¸ªEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/8l/j0ddzz_x75n85vq8tp86dwpr0000gn/T/tmptzmtf89p\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/8l/j0ddzz_x75n85vq8tp86dwpr0000gn/T/tmptzmtf89p', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x120686c18>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Build a DNN with 2 hidden layers and 10 nodes in each hidden layer.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[10, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¦‚æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡æ¥è§¦TensorFlowçš„è¯ï¼Œæ­å–œä½ ï¼Œä½ çš„ç¬¬ä¸€ä¸ªDNNå·²ç»å®šä¹‰å¥½äº† ~\n",
    "<br> è™½ç„¶æˆ‘è§‰å¾—éå¸¸æ˜æ˜¾ï¼Œä½†è¿˜æ˜¯è§£é‡Šä¸€ä¸‹å¥½äº†ï¼Œä½ åˆšåˆšå®šä¹‰äº†ä¸€ä¸ªå¸¦æœ‰ä¸¤å±‚åŒéšå±‚ï¼Œæ¯å±‚æœ‰10ä¸ªunitï¼Œè¾“å‡ºå±‚æœ‰ä¸‰ä¸ªunit(å°±æ˜¯æœ‰ä¸‰ç§ç±»åˆ«)ï¼Œç‰¹å¾åˆ—å°±æ˜¯ä½ åˆšåˆšçœ‹è§çš„my_feature_columns çš„å°ç™½ç‰ˆDNNåˆ†ç±»å™¨ -v-\n",
    "<BR>æ¥ä¸‹æ¥ï¼Œåˆ¶å®šbatch_sizeä»¥åŠtrain_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¥ä¸‹æ¥çš„å·¥ä½œæ˜¯ï¼š\n",
    "#### 1ï¼‰è®­ç»ƒæ¨¡å‹\n",
    "#### 2ï¼‰è¯„ä¼°è®­ç»ƒæ¨¡å‹å‡†ç¡®ç‡\n",
    "#### 3ï¼‰ä½¿ç”¨å·²ç»è®­ç»ƒçš„æ¨¡å‹åšé¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/8l/j0ddzz_x75n85vq8tp86dwpr0000gn/T/tmptzmtf89p/model.ckpt.\n",
      "INFO:tensorflow:loss = 161.19363, step = 1\n",
      "INFO:tensorflow:global_step/sec: 562.711\n",
      "INFO:tensorflow:loss = 11.6809025, step = 101 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 631.298\n",
      "INFO:tensorflow:loss = 9.929311, step = 201 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 557.128\n",
      "INFO:tensorflow:loss = 5.9321713, step = 301 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 573.302\n",
      "INFO:tensorflow:loss = 5.851366, step = 401 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 581.393\n",
      "INFO:tensorflow:loss = 2.5129728, step = 501 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 579.041\n",
      "INFO:tensorflow:loss = 5.1492662, step = 601 (0.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 515.368\n",
      "INFO:tensorflow:loss = 4.1985073, step = 701 (0.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 623.516\n",
      "INFO:tensorflow:loss = 3.329792, step = 801 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 647.139\n",
      "INFO:tensorflow:loss = 5.2057896, step = 901 (0.155 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/8l/j0ddzz_x75n85vq8tp86dwpr0000gn/T/tmptzmtf89p/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.12586.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x1206867f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(\n",
    "    input_fn=lambda:iris_data.train_input_fn(train_x, train_y, batch_size),\n",
    "    steps=train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬ç”¨æµ‹è¯•é›†æ¥è¯„ä¼°è¯¥æ¨¡å‹çš„å‡†ç¡®ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-09-09:16:30\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/8l/j0ddzz_x75n85vq8tp86dwpr0000gn/T/tmptzmtf89p/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-09-09:16:31\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.96666664, average_loss = 0.054533932, global_step = 1000, loss = 1.6360179\n",
      "\n",
      "Test set accuracy: 0.967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y, batch_size = batch_size))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 93.3%çš„å‡†ç¡®ç‡ï¼\n",
    "(å…¶å®ä¸€ç‚¹ä¹Ÿä¸é«˜ ä½ å¾ˆæœ‰å¯èƒ½å¾—åˆ°æ¯”æˆ‘é«˜çš„ç»“æœ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œåšä¸ªé¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions from the model\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1],\n",
    "}\n",
    "\n",
    "predictions = classifier.predict(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                            labels=None,\n",
    "                                            batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Estimator.predict at 0x12104d570>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictè¿”å›çš„æ˜¯ä¸€ä¸ªpythonçš„iterable(âŠ™vâŠ™)\n",
    "<br>ä½ å¯èƒ½å¯¹è¿™ç§æ•°æ®ç±»å‹æœ‰ç–‘é—®\n",
    "<br>ä¹Ÿå¯èƒ½æ²¡æœ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    #print(pred_dict)\n",
    "    template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(iris_data.SPECIES[class_id],\n",
    "                          100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
