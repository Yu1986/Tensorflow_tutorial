{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. å…ˆç®€å•ä»‹ç»ä¸€ä¸‹\n",
    "ï¼ˆä¸çŸ¥é“å¦‚ä½•ä»‹ç»äºæ˜¯ç›´æ¥ç¿»è¯‘å®˜ç½‘äº†-v-ï¼‰\n",
    "<br>tf.dataè¿™ä¸ªæ¨¡å—åŒ…å«ä¸€ç³»åˆ—Classesï¼Œå…è®¸ä½ è½»æ˜“åœ°åŠ è½½æ•°æ®é›†ï¼Œæ“ä½œå®ƒï¼Œç„¶åå°†å®ƒä¼ å…¥åˆ°ä½ çš„æ¨¡å‹ä¸­.\n",
    "<br>è¿™ä¸ªtutorialå‘¢ï¼Œä»ä¸¤ä¸ªç®€å•çš„ä¾‹å­æ¥ä»‹ç»è¿™ä¸ªAPI:\n",
    "* é€šè¿‡Numpyæä¾›çš„arrayï¼Œä»å†…å­˜ä¸­è¯»å…¥æ•°æ®\n",
    "* ä»csvæ–‡ä»¶ä¸­è¯»å…¥è¡Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.è¶…åŸºæœ¬çš„è¾“å…¥\n",
    "ä»æ•°ç»„ä¸­è¯»å–æ˜¯æœ€ç®€å•çš„æ–¹å¼ï¼Œæ‰€ä»¥æˆ‘ä»¬å…ˆä»æœ€ç®€å•çš„å¼€å§‹å¥½å•¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import iris_data\n",
    "\n",
    "train, test = tf.keras.datasets.mnist.load_data()\n",
    "mnist_x, mnist_y = train\n",
    "\n",
    "mnist_ds = tf.data.Dataset.from_tensor_slices(mnist_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. tf.data.Dataset.from_tensor_slices() ä¸­çš„sliceæ˜¯ä»€ä¹ˆæ¦‚å¿µï¼Ÿ\n",
    "é¦–å…ˆå‘¢ï¼Œè¿™ä¸ªå‡½æ•°éœ€è¦æ•°ç»„(numpy.array)ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ç‰‡ä¸€ç‰‡(slice)çš„arrayï¼Œå—¯ï¼Œè¿™ä¸ªä¸€ç‰‡ä¸€ç‰‡çš„æ„æ€å°±æ˜¯ï¼Œæ‹¿ä¸Šé¢çš„Mnistæ•°æ®é›†æ¥è¯´ï¼Œè¾“å…¥çš„æ˜¯å¤§å°ä¸ºï¼ˆ60000ï¼Œ28ï¼Œ28ï¼‰çš„æ•°ç»„ï¼Œè¿”å›çš„æ˜¯60000ç‰‡ï¼Œæ¯ç‰‡å¤§å°ä¸º28*28ï¼Œè¿™æ ·çš„æ•°æ®é›†\n",
    "<br> æˆ‘ä»¬æ¥æ‰“å°ä¸€ä¸‹çœ‹çœ‹ ğŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (28, 28), types: tf.uint8>\n"
     ]
    }
   ],
   "source": [
    "print(mnist_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰“å°å‡ºçš„ä¿¡æ¯æ˜¾ç¤ºäº†è¿™ä¸€ç‰‡çš„å¤§å°ä»¥åŠæ•°æ®ç±»å‹ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸€ç‰‡å¹¶ä¸çŸ¥é“å®ƒæœ‰å¤šå°‘å°ä¼™ä¼´ï¼Œæ„æ€å°±æ˜¯ï¼Œæˆ‘ä»¬æ‰“å°çš„è¿™ä¸€ç‰‡ï¼Œä¸çŸ¥é“ä¸€å…±æœ‰å¤šå°‘ç¯‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. tf.data.Dataset.from_tensor_slices()ä¸­çš„å…¶ä»–å‚æ•°å‘¢ï¼Ÿ\n",
    "arrayç±»å‹çš„å°±å¥½ï¼Œä½†å…¶å®å‡ ä¹ä»»ä½•æ•°æ®ç±»å‹éƒ½å¯ä»¥é€šè¿‡numpy.arrayè¢«è½¬æ¢ä¸ºarrayç±»å‹ï¼Œä¾‹å¦‚ä¸‹é¢è¿™ä¸ªä¾‹å­ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = iris_data.load_data()\n",
    "features, labels = train\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åƒè¿™ç§ç»“æ„åŒ–(dict,array)å…ƒç´ ï¼Œè¿”å›çš„Datasetçš„å¤§å°ï¼ˆshapeï¼‰å’Œç±»å‹ï¼ˆtypeï¼‰é‡‡å–åŒæ ·çš„ç»“æ„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.ç„¶åå°±å¯ä»¥ç”¨äº†å—ï¼Ÿ\n",
    "ä¸ï¼Œè¿˜éœ€è¦è¿›ä¸€æ­¥å¤„ç†ï¼Œç›®å‰è¿”å›çš„Datasetæ¯æ¬¡è¿­ä»£è¿”å›ä¸€ç‰‡æ•°æ®ä¸”ä»¥ä¸€ä¸ªå›ºå®šçš„é¡ºåºï¼Œæ‰€ä»¥ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ“ä½œçš„æœ‰ï¼š\n",
    "* Shuffle æ”¹å˜é¡ºåºç”¨çš„ï¼Œæœ‰ä¸€ä¸ªå‚æ•°æ˜¯buffer_size,ç¡®ä¿æä¾›å¤§äºæ•°æ®é›†çš„ç©ºé—´ï¼Œä»¥ä¾¿å½»åº•æ‰“ä¹±é¡ºåº\n",
    "* Repeat å­—é¢æ„æ€\n",
    "* Batch ä¸€æ¬¡ä¼ å…¥ä¸€å æ•°æ®ï¼ˆå°±æ˜¯ä¼ å…¥å¾ˆå¤šç‰‡çš„æ„æ€ï¼‰ å…¶å®batchæ˜¯ä¸€ä¸ªçŸ©é˜µåŒ–ï¼Œæ¶‰åŠåˆ°æ•ˆç‡é—®é¢˜ï¼Œè™½ç„¶å¾ˆé‡è¦ï¼Œä½†æ˜¯å†™èµ·æ¥å¤ªéº»çƒ¦äº†ï¼Œä½†è¿˜æ˜¯æ¨èçœ‹ä¸€ä¸‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShuffleDataset shapes: (28, 28), types: tf.uint8>\n"
     ]
    }
   ],
   "source": [
    "# æ˜¾ç„¶æ˜¯æ²¡æœ‰ç”¨ä¸”æ²¡æ•ˆæœçš„æ‰“ä¹±\n",
    "print(mnist_ds.shuffle(100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (?, 28, 28), types: tf.uint8>\n"
     ]
    }
   ],
   "source": [
    "#ä¸€æ¬¡ä¼ å…¥ä¸€ç™¾ç‰‡ -v-\n",
    "print(mnist_ds.batch(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸è¿‡ä½ å¤§æ¦‚æ³¨æ„åˆ°ï¼Œè¾“å‡ºçš„å¤§å°æ˜¯(?,28,28),è€Œä¸æ˜¯(100,28,28),æ˜¯å› ä¸ºï¼Œæœ€åä¸€æ¬¡ä¼ å…¥æ•°æ®æ—¶ï¼Œä¹Ÿè®¸æ²¡åŠæ³•å‡‘æ•´ï¼Œä¼šå°ä¸€ç‚¹ï¼Œæ‰€ä»¥è¿™ä¸ªå¤§å°æ²¡åŠæ³•ç¡®å®šï¼Œå°±ç”¨ï¼Ÿæ¥ä»£æ›¿äº†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.ä½†æ˜¯åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œä¹Ÿéœ€è¦ç‰¹å¾å’Œæ ‡ç­¾ï¼Œè¯¥å¦‚ä½•åŠ å…¥è¿”å›çš„æ•°æ®é›†å‘¢ï¼Ÿ\n",
    "å°±æ˜¯ï¼Œä¾‹å¦‚ä»é¢œè‰²å¤§å°é‡é‡æ¥åˆ¤æ–­æ˜¯ä¸æ˜¯è‹¹æœï¼Œé‚£ä¹ˆæ¨¡å‹éœ€è¦çŸ¥é“çªç„¶ä¼ å…¥çš„å››ä¸ªæ•°å€¼ï¼Œè°æ˜¯é¢œè‰²å¤§å°é‡é‡å’Œæ ‡ç­¾...\n",
    "<br>datasetæä¾›ä»¥ä¸‹è¿™ç§æ–¹å¼å°†æ•°æ®è½¬åŒ–ä¸ºéœ€è¦çš„æ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the Iterator, and return the read end of the pipeline.\n",
    "features_result, labels_result = dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'SepalLength': <tf.Tensor 'IteratorGetNext_1:2' shape=() dtype=float64>, 'SepalWidth': <tf.Tensor 'IteratorGetNext_1:3' shape=() dtype=float64>, 'PetalLength': <tf.Tensor 'IteratorGetNext_1:0' shape=() dtype=float64>, 'PetalWidth': <tf.Tensor 'IteratorGetNext_1:1' shape=() dtype=float64>}, <tf.Tensor 'IteratorGetNext_1:4' shape=() dtype=int64>)\n"
     ]
    }
   ],
   "source": [
    "print((features_result, labels_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.å¦ä¸€ç§è¯»å…¥æ•°æ®æ–¹å¼ ---- ä»CSVæ–‡ä»¶ä¸­è¯»å…¥æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, test_path = iris_data.maybe_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å»ºç«‹æ•°æ®é›†\n",
    "é¦–å…ˆæˆ‘ä»¬å»ºç«‹ä¸€ä¸ªTextLineDatasetå¯¹è±¡ï¼Œæ¯æ¬¡ä»æ–‡ä»¶ä¸­è¯»å…¥ä¸€è¡Œæ•°æ®ã€‚\n",
    "<br>ç„¶åï¼Œæˆ‘ä»¬è°ƒç”¨skipæ–¹æ³•ï¼Œæ¥è·³è¿‡æ–‡ä»¶çš„ç¬¬ä¸€è¡Œï¼ˆå› ä¸ºåªæ˜¯ä¸ªæ ‡é¢˜ï¼Œä¸æ˜¯æ•°æ®ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(train_path).skip(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å»ºç«‹csvåˆ†å‰²å…ƒ\n",
    "æˆ‘ä»¬éœ€è¦å°†æ¯ä¸€è¡Œåˆ†å‰²ä¸ºï¼ˆç‰¹å¾ï¼Œæ ‡ç­¾ï¼‰å¯¹\n",
    "<br>æ‰€ä»¥ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®è‡ªå·±çš„éœ€æ±‚ï¼Œå»ºç«‹åˆ†å‰²å…ƒ\n",
    "<br>å®˜ç½‘ä¸Šç»™äº†æ ·ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Metadata describing the text columns\n",
    "COLUMNS = ['SepalLength', 'SepalWidth',\n",
    "           'PetalLength', 'PetalWidth',\n",
    "           'label']\n",
    "FIELD_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0]]\n",
    "def _parse_line(line):\n",
    "    # Decode the line into its fields\n",
    "    fields = tf.decode_csv(line, FIELD_DEFAULTS)\n",
    "\n",
    "    # Pack the result into a dictionary\n",
    "    features = dict(zip(COLUMNS,fields))\n",
    "\n",
    "    # Separate the label from the features\n",
    "    labels= features.pop('label')\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href = \"https://www.tensorflow.org/api_docs/python/tf/decode_csv?hl=zh-cn\">tf.decode_csv()</a>: å°†CSVè®°å½•è½¬åŒ–ä¸ºtensors <br>\n",
    "* zip()ï¼šæ˜¯pythonæä¾›çš„æ‰“åŒ…å‡½æ•°ï¼Œä¾‹å¦‚a = [1,2,3],b = [4,5,6],é‚£ä¹ˆè¿”å›çš„zip(a,b)å°±æ˜¯[(1,4),(2,5),(3,6)],å—¯ä¹Ÿå°±æ˜¯ï¼Œé¦–å…ˆæ‰“åŒ…æˆå…ƒç»„ï¼Œå†è¿”å›ç”±å…ƒç»„ç»„æˆçš„åˆ—è¡¨\n",
    "* pop():é˜Ÿåˆ—ä¸­çš„pop(),å¯ä»¥ç†è§£ä¸ºå‡ºé˜Ÿï¼Œä»featuresä¸­æ‰¾åˆ°åä¸ºâ€œlabelâ€çš„è´§ï¼Œè®©å®ƒè½¬é˜Ÿåˆ°labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float32, SepalWidth: tf.float32, PetalLength: tf.float32, PetalWidth: tf.float32}, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(_parse_line)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* map()æ˜¯Datasetè‡ªå¸¦çš„å‡½æ•°ï¼Œéœ€è¦ä¼ å…¥çš„å‚æ•°æ˜¯è‡ªå®šä¹‰çš„æ˜ å°„å‡½æ•°ï¼Œä¾‹å¦‚æˆ‘ä»¬åˆšåˆšå®šä¹‰çš„_parse_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šï¼Œè¿™ä¸€éƒ¨åˆ†çš„tutorialç»“æŸå•¦=w="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
